"""
4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸: Generate â†’ Critique â†’ Filter â†’ Pack
Dyarchy ë¡œì»¬ LLM í†µí•©ì„ ìœ„í•œ ìµœì í™”ëœ ì•„í‚¤í…ì²˜
"""

import asyncio
import hashlib
import json
import re
import time
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import yaml
from pydantic import BaseModel, Field

# ìŠ¤í‚¤ë§ˆ ì„í¬íŠ¸
try:
    from ..schemas.document import DocumentMetadata
    from ..schemas.training import ContextItem
except ImportError:
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© ê¸°ë³¸ í´ë˜ìŠ¤
    class ContextItem(BaseModel):
        content: str
        metadata: Dict[str, Any] = Field(default_factory=dict)

    class DocumentMetadata(BaseModel):
        source: str
        page: int
        coordinates: Optional[Dict[str, Any]] = None


class TaskType(str, Enum):
    """ì‘ì—… ìœ í˜•"""

    SUMMARIZE = "summarize"
    EXTRACT = "extract"
    ANALYZE = "analyze"
    TRANSLATE = "translate"
    QA = "qa"
    REASONING = "reasoning"


class DomainType(str, Enum):
    """ë„ë©”ì¸ ìœ í˜•"""

    INSURANCE = "insurance"
    LEGAL = "legal"
    MEDICAL = "medical"
    FINANCIAL = "financial"
    GENERAL = "general"


class KoreanConstraint(str, Enum):
    """í•œêµ­ì–´ ì œì•½ ì¡°ê±´"""

    PII_FILTER = "pii_filter"  # ê°œì¸ì •ë³´ í•„í„°ë§
    FORMAL_SPEECH = "formal_speech"  # ê²©ì‹ì²´ ì‚¬ìš©
    HONORIFICS = "honorifics"  # ì¡´ëŒ“ë§ ì ì ˆì„±
    TERMINOLOGY = "terminology"  # ì „ë¬¸ìš©ì–´ í†µì¼
    NO_SLUR = "no_slur"  # ë¹„ì†ì–´ ê¸ˆì§€
    LENGTH_LIMIT = "length_limit"  # ê¸¸ì´ ì œí•œ


@dataclass
class PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì •"""

    max_tokens_input: int = 8000
    max_tokens_output: int = 2048
    temperature: float = 0.7
    max_retries: int = 2
    batch_size: int = 50
    enable_korean_checks: bool = True
    korean_constraints: List[KoreanConstraint] = None

    def __post_init__(self):
        if self.korean_constraints is None:
            self.korean_constraints = [
                KoreanConstraint.PII_FILTER,
                KoreanConstraint.FORMAL_SPEECH,
                KoreanConstraint.HONORIFICS,
                KoreanConstraint.TERMINOLOGY,
                KoreanConstraint.NO_SLUR,
                KoreanConstraint.LENGTH_LIMIT,
            ]


# Backward-compat alias to distinguish from conversion pipeline config.
TrainingPipelineConfig = PipelineConfig


class GeneratedExample(BaseModel):
    """ìƒì„±ëœ ì˜ˆì œ"""

    id: str = Field(..., description="ê³ ìœ  ID")
    task_type: TaskType = Field(..., description="ì‘ì—… ìœ í˜•")
    domain_type: DomainType = Field(..., description="ë„ë©”ì¸ ìœ í˜•")

    # ì…ë ¥
    instruction: str = Field(..., description="ì§€ì‹œë¬¸")
    input_context: List[ContextItem] = Field(..., description="ì…ë ¥ ì»¨í…ìŠ¤íŠ¸")

    # ì¶œë ¥
    output: str = Field(..., description="ìƒì„±ëœ ì‘ë‹µ")
    thinking: Optional[str] = Field(None, description="ìƒê° ê³¼ì •")

    # ë©”íƒ€ë°ì´í„°
    model_name: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸")
    temperature: float = Field(..., description="ì‚¬ìš©ëœ ì˜¨ë„")
    tokens_used: int = Field(..., description="ì‚¬ìš©ëœ í† í° ìˆ˜")
    generation_time: float = Field(..., description="ìƒì„± ì‹œê°„")

    # í’ˆì§ˆ í‰ê°€
    quality_score: Optional[float] = Field(None, description="í’ˆì§ˆ ì ìˆ˜")
    passed_korean_checks: bool = Field(True, description="í•œêµ­ì–´ ê²€ì‚¬ í†µê³¼ ì—¬ë¶€")
    constraint_violations: List[str] = Field(default_factory=list, description="ìœ„ë°˜ëœ ì œì•½ì¡°ê±´")

    # ìƒì„± ì •ë³´
    created_at: str = Field(..., description="ìƒì„± ì‹œê°„")
    batch_id: str = Field(..., description="ë°°ì¹˜ ID")
    hash: str = Field(..., description="í•´ì‹œê°’")


class CritiqueResult(BaseModel):
    """ë¹„í‰ ê²°ê³¼"""

    example_id: str = Field(..., description="ì˜ˆì œ ID")

    # í’ˆì§ˆ í‰ê°€ (1-10 ì ìˆ˜)
    coherence: float = Field(..., description="ì¼ê´€ì„± (1-10)")
    accuracy: float = Field(..., description="ì •í™•ì„± (1-10)")
    completeness: float = Field(..., description="ì™„ì „ì„± (1-10)")
    korean_quality: float = Field(..., description="í•œêµ­ì–´ í’ˆì§ˆ (1-10)")

    # í‰ê°€ ìš”ì¸
    reasoning_quality: Optional[float] = Field(None, description="ì¶”ë¡  í’ˆì§ˆ")
    domain_knowledge: Optional[float] = Field(None, description="ë„ë©”ì¸ ì§€ì‹")
    clarity: float = Field(..., description="ëª…í™•ì„± (1-10)")

    # í†µê³¼ ì—¬ë¶€
    overall_score: float = Field(..., description="ì¢…í•© ì ìˆ˜ (1-10)")
    passed: bool = Field(..., description="í†µê³¼ ì—¬ë¶€ (7ì  ì´ìƒ)")
    reasoning: str = Field(..., description="í‰ê°€ ì´ìœ ")

    # ê°œì„  ì œì•ˆ
    suggestions: List[str] = Field(default_factory=list, description="ê°œì„  ì œì•ˆ")

    # í‰ê°€ ì •ë³´
    critic_model: str = Field(..., description="ë¹„í‰ ëª¨ë¸")
    evaluation_time: float = Field(..., description="í‰ê°€ ì‹œê°„")
    created_at: str = Field(..., description="í‰ê°€ ì‹œê°„")


class FilterCriteria(BaseModel):
    """í•„í„°ë§ ê¸°ì¤€"""

    min_overall_score: float = Field(7.0, description="ìµœì†Œ ì¢…í•© ì ìˆ˜")
    min_korean_quality: float = Field(6.0, description="ìµœì†Œ í•œêµ­ì–´ í’ˆì§ˆ ì ìˆ˜")
    max_constraint_violations: int = Field(2, description="ìµœëŒ€ ì œì•½ì¡°ê±´ ìœ„ë°˜ ìˆ˜")
    must_pass_korean_checks: bool = Field(True, description="í•œêµ­ì–´ ê²€ì‚¬ í†µê³¼ í•„ìˆ˜")
    allow_duplicate_content: bool = Field(False, description="ì¤‘ë³µ ë‚´ìš© í—ˆìš© ì—¬ë¶€")


class KoreanQualityChecker:
    """í•œêµ­ì–´ í’ˆì§ˆ ê²€ì‚¬ê¸°"""

    def __init__(self):
        # PII íŒ¨í„´ (ê°„ë‹¨í™”ëœ ì˜ˆì‹œ)
        self.pii_patterns = [
            r"\d{2,4}[-]\d{2,4}[-]\d{2,4}",  # ìƒë…„ì›”ì¼
            r"\d{3}-\d{2}-\d{4}",  # ì£¼ë¯¼ë²ˆí˜¸
            r"\d{2,3}-\d{3,4}-\d{4}",  # ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸
            r"01[016]-\d{3,4}-\d{7}",  # íœ´ëŒ€í°
            r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",  # ì´ë©”ì¼
        ]

        # ë¹„ì†ì–´ ëª©ë¡ (ê°„ë‹¨í™”ëœ ì˜ˆì‹œ)
        self.slur_words = [
            "ì”¨ë°œ",
            "ê°œìƒˆë¼",
            "ë¯¸ì¹œ",
            "ë³‘ì‹ ",
            "ì¡´ë‚˜ê²Œ",
            "ë†ˆ",
            "ì¢†",
            "ë¯¸ì¹œë†ˆ",
            "ìŒ‰ë…„",
            "í•œì‹¬",
        ]

        # ê²©ì‹ì²´ í‘œí˜„
        self.formal_markers = ["~ì…ë‹ˆë‹¤", "~ë‹ˆë‹¤", "~í•´ì•¼ í•©ë‹ˆë‹¤", "~í•˜ì‹­ì‹œì˜¤"]

        # ì¡´ëŒ“ë§ ê²€ì¦ì„ ìœ„í•œ ê¸°ë³¸ ì¡´ì¹­
        self.honorifics = ["ë‹˜", "ì”¨", "ì„ ìƒë‹˜", "êµìˆ˜ë‹˜", "ë°•ì‚¬ë‹˜"]

    def check_pii(self, text: str) -> bool:
        """ê°œì¸ì •ë³´ í¬í•¨ ì—¬ë¶€ ê²€ì‚¬"""
        for pattern in self.pii_patterns:
            if re.search(pattern, text):
                return True
        return False

    def check_slur(self, text: str) -> bool:
        """ë¹„ì†ì–´ í¬í•¨ ì—¬ë¶€ ê²€ì‚¬"""
        return any(slur in text for slur in self.slur_words)

    def check_formality(self, text: str) -> Tuple[bool, str]:
        """ê²©ì‹ì²´ ì‚¬ìš© ì—¬ë¶€ ê²€ì‚¬"""
        has_formal = any(marker in text for marker in self.formal_markers)
        if has_formal:
            return True, "ì ì ˆí•œ ê²©ì‹ì²´ ì‚¬ìš©"
        else:
            return False, "ê²©ì‹ì²´ ì‚¬ìš© í•„ìš”"

    def check_honorifics(self, text: str) -> Tuple[bool, str]:
        """ì¡´ëŒ“ë§ ì ì ˆì„± ê²€ì‚¬"""
        # ê°„ë‹¨í™”ëœ ê²€ì‚¬ - ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¬¸ë§¥ ë¶„ì„ í•„ìš”
        context_words = text.split()
        has_honorific = any(honorific in context_words for honorific in self.honorifics)

        # ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ì¡´ëŒ“ë§ì´ ì ì ˆí•  ìˆ˜ ìˆìŒ
        if has_honorific:
            return True, "ì¡´ëŒ“ë§ ì ì ˆíˆ ì‚¬ìš©ë¨"

        # ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œë° ì¡´ëŒ“ë§ì´ ì—†ìœ¼ë©´ ë¶€ì ì ˆí•  ìˆ˜ ìˆìŒ
        if "ì§ˆë¬¸" in text or "ë¬¸ì˜" in text:
            return False, "ì§ˆë¬¸/ë¬¸ì˜ ë§¥ë½ì—ì„œ ì¡´ëŒ“ë§ ì‚¬ìš© í•„ìš”"

        return True, "ì¡´ëŒ“ë§ ì ì ˆí•¨"

    def check_length(self, text: str, max_length: int = 5000) -> Tuple[bool, str]:
        """ê¸¸ì´ ì œí•œ ê²€ì‚¬"""
        if len(text) > max_length:
            return False, f"í…ìŠ¤íŠ¸ ê¸¸ì´ ì´ˆê³¼ ({len(text)} > {max_length})"
        return True, "ì ì ˆí•œ ê¸¸ì´"

    def check_all_constraints(
        self, text: str, constraints: List[KoreanConstraint], config: PipelineConfig
    ) -> Dict[str, Any]:
        """ëª¨ë“  ì œì•½ì¡°ê±´ ê²€ì‚¬"""
        results = {"passed": True, "violations": [], "suggestions": []}

        for constraint in constraints:
            if constraint == KoreanConstraint.PII_FILTER:
                has_pii = self.check_pii(text)
                if has_pii:
                    results["passed"] = False
                    results["violations"].append("PII ì •ë³´ í¬í•¨")
                    results["suggestions"].append("ê°œì¸ì •ë³´ë¥¼ ë§ˆìŠ¤í‚¹í•˜ê±°ë‚˜ ì œê±°í•˜ì„¸ìš”")

            elif constraint == KoreanConstraint.NO_SLUR:
                has_slur = self.check_slur(text)
                if has_slur:
                    results["passed"] = False
                    results["violations"].append("ë¶€ì ì ˆí•œ í‘œí˜„ í¬í•¨")
                    results["suggestions"].append("ë¹„ì†ì–´ë¥¼ ì œê±°í•˜ê³  ì ì ˆí•œ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”")

            elif constraint == KoreanConstraint.FORMAL_SPEECH:
                is_formal, message = self.check_formality(text)
                if not is_formal:
                    results["passed"] = False
                    results["violations"].append("ë¹„ê²©ì‹ì²´")
                    results["suggestions"].append(message)

            elif constraint == KoreanConstraint.HONORIFICS:
                is_appropriate, message = self.check_honorifics(text)
                if not is_appropriate:
                    results["violations"].append("ì¡´ëŒ“ë§ ë¶€ì ì ˆ")
                    results["suggestions"].append(message)

            elif constraint == KoreanConstraint.LENGTH_LIMIT:
                is_appropriate, message = self.check_length(text, config.max_tokens_output * 4)
                if not is_appropriate:
                    results["violations"].append("ê¸¸ì´ ì œí•œ ì´ˆê³¼")
                    results["suggestions"].append(message)

        return results


class FourStagePipeline:
    """4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸: Generate â†’ Critique â†’ Filter â†’ Pack"""

    def __init__(self, config: PipelineConfig):
        self.config = config
        self.korean_checker = KoreanQualityChecker()

        # ì €ì¥ ë””ë ‰í† ë¦¬
        self.output_dir = Path("./generated_data")
        self.output_dir.mkdir(exist_ok=True)

        # ì§„í–‰ ìƒíƒœ ì¶”ì 
        self.current_batch = []
        self.processed_examples = []

        # ì¤‘ë³µ ê²€ì‚¬ìš© í•´ì‹œ ì„¸íŠ¸
        self._seen_hashes: set = set()

    async def generate_examples(
        self,
        instructions: List[str],
        contexts: List[List[ContextItem]],
        task_type: TaskType,
        domain_type: DomainType,
    ) -> List[GeneratedExample]:
        """1ë‹¨ê³„: ì˜ˆì œ ìƒì„±"""
        print(f"ğŸ“ 1ë‹¨ê³„: ì˜ˆì œ ìƒì„± ì‹œì‘ ({len(instructions)}ê°œ)")

        examples = []
        batch_id = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]

        # Dyarchy LLM í˜¸ì¶œ (ê°€ìƒ - ì‹¤ì œë¡œëŠ” Dyarchy API í˜¸ì¶œ)
        for i, (instruction, context) in enumerate(zip(instructions, contexts)):
            start_time = time.time()

            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì—¬ê¸°ì— Dyarchy LLM í˜¸ì¶œ
            # í˜„ì¬ëŠ” ëª¨ì˜ ìƒì„±
            thinking = self._generate_thinking(instruction, context)
            output = self._generate_output(instruction, context, thinking)

            example = GeneratedExample(
                id=f"gen_{batch_id}_{i:03d}",
                task_type=task_type,
                domain_type=domain_type,
                instruction=instruction,
                input_context=context,
                output=output,
                thinking=thinking,
                model_name="local-llm",
                temperature=self.config.temperature,
                tokens_used=len(output.split()),
                generation_time=time.time() - start_time,
                created_at=time.strftime("%Y-%m-%d %H:%M:%S"),
                batch_id=batch_id,
                hash=hashlib.md5(output.encode()).hexdigest(),
            )

            # í•œêµ­ì–´ ê²€ì‚¬
            korean_results = self.korean_checker.check_all_constraints(
                output + (thinking or ""), self.config.korean_constraints, self.config
            )

            example.passed_korean_checks = korean_results["passed"]
            example.constraint_violations = korean_results["violations"]

            examples.append(example)

        print(f"âœ… ìƒì„± ì™„ë£Œ: {len(examples)}ê°œ ì˜ˆì œ")
        return examples

    def _generate_thinking(self, instruction: str, context: List[ContextItem]) -> str:
        """ìƒê° ê³¼ì • ìƒì„± (ëª¨ì˜)"""
        context_text = "\n".join([item.content for item in context])

        thinking = f"""ìƒê° ê³¼ì •:
1. ì‚¬ìš©ì ìš”ì²­ ë¶„ì„: "{instruction}"
2. ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ í™•ì¸: {len(context)}ê°œ í•­ëª© ì œê³µë¨
3. í•µì‹¬ ì •ë³´ ì¶”ì¶œ: {context_text[:200]}...
4. ë…¼ë¦¬ì  ì‚¬ê³ : ë³´í—˜/ë„ë©”ì¸ ì§€ì‹ ì ìš©
5. êµ¬ì¡°í™”ëœ ë‹µë³€ ì‘ì„±"""

        return thinking

    def _generate_output(self, instruction: str, context: List[ContextItem], thinking: str) -> str:
        """ìµœì¢… ì¶œë ¥ ìƒì„± (ëª¨ì˜)"""
        context_text = "\n".join([item.content for item in context])

        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Dyarchy LLM í˜¸ì¶œ
        output = f"""ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ìš”ì²­: {instruction}

ê´€ë ¨ ì •ë³´:
{context_text}

ë¶„ì„ ê²°ê³¼:
{thinking}

ìµœì¢… ë‹µë³€:
ë³´í—˜ ë„ë©”ì¸ ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.
"""

        return output

    async def critique_examples(self, examples: List[GeneratedExample]) -> List[CritiqueResult]:
        """2ë‹¨ê³„: í’ˆì§ˆ ë¹„í‰"""
        print(f"ğŸ” 2ë‹¨ê³„: í’ˆì§ˆ ë¹„í‰ ì‹œì‘ ({len(examples)}ê°œ)")

        critiques = []

        for example in examples:
            time.time()

            # Dyarchy ë¹„í‰ ëª¨ë¸ í˜¸ì¶œ (ê°€ìƒ)
            critique = self._generate_critique(example)

            critiques.append(critique)

        print(f"âœ… ë¹„í‰ ì™„ë£Œ: {len(critiques)}ê°œ í‰ê°€")
        return critiques

    def _generate_critique(self, example: GeneratedExample) -> CritiqueResult:
        """í’ˆì§ˆ ë¹„í‰ ìƒì„± (ëª¨ì˜)"""
        # ê°„ë‹¨í™”ëœ í‰ê°€ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ í‰ê°€ ëª¨ë¸ ì‚¬ìš©)

        # ì¼ê´€ì„± í‰ê°€
        coherence_score = self._evaluate_coherence(example.output)

        # ì •í™•ì„± í‰ê°€
        accuracy_score = self._evaluate_accuracy(example.output, example.input_context)

        # ì™„ì „ì„± í‰ê°€
        completeness_score = self._evaluate_completeness(example.output, example.instruction)

        # í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€
        korean_quality_score = self._evaluate_korean_quality(example.output)

        # ëª…í™•ì„± í‰ê°€
        clarity_score = self._evaluate_clarity(example.output)

        # ì¢…í•© ì ìˆ˜
        overall_score = (
            coherence_score + accuracy_score + completeness_score + korean_quality_score + clarity_score
        ) / 5

        passed = overall_score >= 7.0

        # í‰ê°€ ì´ìœ 
        reasoning = self._generate_reasoning(overall_score, passed)

        return CritiqueResult(
            example_id=example.id,
            coherence=coherence_score,
            accuracy=accuracy_score,
            completeness=completeness_score,
            korean_quality=korean_quality_score,
            clarity=clarity_score,
            overall_score=overall_score,
            passed=passed,
            reasoning=reasoning,
            critic_model="critic-model",
            evaluation_time=0.1,
            created_at=time.strftime("%Y-%m-%d %H:%M:%S"),
        )

    def _evaluate_coherence(self, text: str) -> float:
        """ì¼ê´€ì„± í‰ê°€ (1-10)"""
        # ê°„ë‹¨í™”ëœ í‰ê°€
        if len(text) < 50:
            return 3.0

        sentences = text.split(".")
        if len(sentences) < 2:
            return 5.0

        # ë¬¸ì¥ ê°„ ì—°ê²°ì„±
        coherence_indicators = ["ê·¸ëŸ¬ë‚˜", "ë”°ë¼ì„œ", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ì´ ë•Œë¬¸ì—"]
        coherence_count = sum(
            1 for sentence in sentences if any(indicator in sentence for indicator in coherence_indicators)
        )

        score = min(10.0, 3.0 + coherence_count * 0.5)
        return score

    def _evaluate_accuracy(self, output: str, context: List[ContextItem]) -> float:
        """ì •í™•ì„± í‰ê°€ (1-10)"""
        # ê°„ë‹¨í™”ëœ í‰ê°€ - ì‹¤ì œë¡œëŠ” ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í‰ê°€ í•„ìš”
        context_text = " ".join([item.content for item in context])

        # ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ê²€ì‚¬
        common_words = set(context_text.split()) & set(output.split())
        coverage = len(common_words) / max(len(set(output.split())), 1)

        score = 4.0 + coverage * 6.0
        return min(10.0, score)

    def _evaluate_completeness(self, output: str, instruction: str) -> float:
        """ì™„ì „ì„± í‰ê°€ (1-10)"""
        # ê°„ë‹¨í™”ëœ í‰ê°€
        if len(output) < 100:
            return 3.0

        # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ completeness
        question_words = set(instruction.split())
        answer_words = set(output.split())

        overlap = len(question_words & answer_words)
        completeness = overlap / max(len(question_words), 1)

        score = 3.0 + completeness * 7.0
        return min(10.0, score)

    def _evaluate_korean_quality(self, text: str) -> float:
        """í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ (1-10)"""
        # ë¬¸ë²•ì  ì™„ì„±ì„±, ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€
        # ê°„ë‹¨í™”ëœ í‰ê°€

        # ê¸°ë³¸ì ì¸ í•œêµ­ì–´ êµ¬ì¡° í™•ì¸
        korean_patterns = [
            "ì€/ëŠ”",
            "ì´/ê°€",
            "ì„/ë¥¼",
            "ì˜",
            "ì—",
            "ì—ì„œ",
            "ìœ¼ë¡œ",
            "ê¹Œì§€",
        ]

        pattern_count = sum(1 for pattern in korean_patterns if pattern.replace("/", "") in text)

        # ë¬¸ì¥ ë¶€í˜¸ì‚¬ìš©
        punctuation_score = min(2.0, text.count(".") + text.count("!") + text.count("?"))

        score = 4.0 + (pattern_count / len(korean_patterns)) * 2.0 + punctuation_score
        return min(10.0, score)

    def _evaluate_clarity(self, text: str) -> float:
        """ëª…í™•ì„± í‰ê°€ (1-10)"""
        # ê°„ë‹¨í™”ëœ í‰ê°€
        if len(text) < 20:
            return 3.0

        # í‰ê·  ë¬¸ì¥ ê¸¸ì´
        sentences = [s.strip() for s in text.split(".") if s.strip()]
        if not sentences:
            return 3.0

        avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)

        # ë„ˆë¬´ ê¸¸ê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ íŒ¨ë„í‹°
        if avg_sentence_length > 200:
            clarity_penalty = 2.0
        elif avg_sentence_length < 10:
            clarity_penalty = 2.0
        else:
            clarity_penalty = 0.0

        score = 8.0 - clarity_penalty
        return max(1.0, score)

    def _generate_reasoning(self, overall_score: float, passed: bool) -> str:
        """í‰ê°€ ì´ìœ  ìƒì„±"""
        if passed:
            if overall_score >= 9.0:
                return "ë§¤ìš° ìš°ìˆ˜í•œ í’ˆì§ˆì„ ë³´ì…ë‹ˆë‹¤."
            elif overall_score >= 8.0:
                return "ìš°ìˆ˜í•œ í’ˆì§ˆì„ ë³´ì…ë‹ˆë‹¤."
            else:
                return "ì–‘í˜¸í•œ í’ˆì§ˆì„ ë³´ì…ë‹ˆë‹¤."
        else:
            if overall_score >= 6.0:
                return "ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
            elif overall_score >= 4.0:
                return "ìƒë‹¹í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
            else:
                return "ì „ë©´ì ì¸ ì¬ì‘ì„±ì´ í•„ìš”í•©ë‹ˆë‹¤."

    async def filter_examples(
        self,
        examples: List[GeneratedExample],
        critiques: List[CritiqueResult],
        criteria: FilterCriteria,
    ) -> Tuple[List[GeneratedExample], List[GeneratedExample]]:
        """3ë‹¨ê³„: ì˜ˆì œ í•„í„°ë§"""
        print(f"ğŸ” 3ë‹¨ê³„: ì˜ˆì œ í•„í„°ë§ ì‹œì‘ ({len(examples)}ê°œ)")

        passed_examples = []
        failed_examples = []

        for example, critique in zip(examples, critiques):
            # ë¹„í‰ ê²°ê³¼ì™€ ì˜ˆì œ ë§¤í•‘
            example.quality_score = critique.overall_score
            example.passed_korean_checks = example.passed_korean_checks

            # í•„í„°ë§ ì¡°ê±´ í™•ì¸
            passes_filter = self._check_filter_criteria(example, critique, criteria)

            if passes_filter:
                passed_examples.append(example)
            else:
                failed_examples.append(example)

        print(f"âœ… í•„í„°ë§ ì™„ë£Œ: í†µê³¼ {len(passed_examples)}ê°œ, ì œì™¸ {len(failed_examples)}ê°œ")
        return passed_examples, failed_examples

    def _check_filter_criteria(
        self,
        example: GeneratedExample,
        critique: CritiqueResult,
        criteria: FilterCriteria,
    ) -> bool:
        """í•„í„°ë§ ì¡°ê±´ í™•ì¸"""

        # ì¢…í•© ì ìˆ˜ í™•ì¸
        if critique.overall_score < criteria.min_overall_score:
            return False

        # í•œêµ­ì–´ í’ˆì§ˆ í™•ì¸
        if critique.korean_quality < criteria.min_korean_quality:
            return False

        # í•œêµ­ì–´ ê²€ì‚¬ í†µê³¼ ì—¬ë¶€ í™•ì¸
        if criteria.must_pass_korean_checks and not example.passed_korean_checks:
            return False

        # ì œì•½ì¡°ê±´ ìœ„ë°˜ íšŸìˆ˜ í™•ì¸
        if len(example.constraint_violations) > criteria.max_constraint_violations:
            return False

        # ì¤‘ë³µ ë‚´ìš© í™•ì¸ (í•´ì‹œ ê¸°ë°˜)
        if not criteria.allow_duplicate_content:
            content_hash = hashlib.md5((example.instruction + example.output).encode()).hexdigest()
            if content_hash in self._seen_hashes:
                return False
            self._seen_hashes.add(content_hash)

        return True

    async def pack_examples(self, examples: List[GeneratedExample], format_type: str = "yaml") -> str:
        """4ë‹¨ê³„: ì˜ˆì œ íŒ¨í‚¹"""
        print(f"ğŸ“¦ 4ë‹¨ê³„: ì˜ˆì œ íŒ¨í‚¹ ì‹œì‘ ({len(examples)}ê°œ)")

        if format_type.lower() == "yaml":
            return self._pack_yaml(examples)
        elif format_type.lower() == "jsonl":
            return self._pack_jsonl(examples)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í¬ë§·: {format_type}")

    def _pack_yaml(self, examples: List[GeneratedExample]) -> str:
        """YAML í¬ë§·ìœ¼ë¡œ íŒ¨í‚¹"""
        yaml_data = []

        for example in examples:
            yaml_example = {
                "instruction": example.instruction,
                "input": [ctx.dict() for ctx in example.input_context],
                "output": example.output,
                "metadata": {
                    "task_type": example.task_type,
                    "domain_type": example.domain_type,
                    "model": example.model_name,
                    "quality_score": example.quality_score,
                    "tokens_used": example.tokens_used,
                    "generation_time": example.generation_time,
                    "passed_korean_checks": example.passed_korean_checks,
                    "constraint_violations": example.constraint_violations,
                    "created_at": example.created_at,
                    "batch_id": example.batch_id,
                    "hash": example.hash,
                },
            }

            if example.thinking:
                yaml_example["metadata"]["thinking"] = example.thinking

            yaml_data.append(yaml_example)

        return yaml.dump(yaml_data, default_flow_style=False, allow_unicode=True)

    def _pack_jsonl(self, examples: List[GeneratedExample]) -> str:
        """JSONL í¬ë§·ìœ¼ë¡œ íŒ¨í‚¹"""
        jsonl_lines = []

        for example in examples:
            jsonl_example = {
                "instruction": example.instruction,
                "input": [ctx.dict() for ctx in example.input_context],
                "output": example.output,
                "metadata": {
                    "task_type": example.task_type,
                    "domain_type": example.domain_type,
                    "model": example.model_name,
                    "quality_score": example.quality_score,
                    "tokens_used": example.tokens_used,
                    "generation_time": example.generation_time,
                    "passed_korean_checks": example.passed_korean_checks,
                    "constraint_violations": example.constraint_violations,
                    "created_at": example.created_at,
                    "batch_id": example.batch_id,
                    "hash": example.hash,
                },
            }

            if example.thinking:
                jsonl_example["metadata"]["thinking"] = example.thinking

            jsonl_lines.append(json.dumps(jsonl_example, ensure_ascii=False))

        return "\n".join(jsonl_lines)

    async def run_full_pipeline(
        self,
        instructions: List[str],
        contexts: List[List[ContextItem]],
        task_type: TaskType,
        domain_type: DomainType,
        filter_criteria: Optional[FilterCriteria] = None,
        output_format: str = "yaml",
    ) -> str:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘")
        start_time = time.time()

        # ê¸°ë³¸ í•„í„°ë§ ê¸°ì¤€
        if filter_criteria is None:
            filter_criteria = FilterCriteria()

        try:
            # 1ë‹¨ê³„: ìƒì„±
            examples = await self.generate_examples(instructions, contexts, task_type, domain_type)

            # 2ë‹¨ê³„: ë¹„í‰
            critiques = await self.critique_examples(examples)

            # ë¹„í‰ ê²°ê³¼ë¥¼ ì˜ˆì œì— ë§¤í•‘
            for example, critique in zip(examples, critiques):
                example.quality_score = critique.overall_score

            # 3ë‹¨ê³„: í•„í„°ë§
            passed_examples, failed_examples = await self.filter_examples(examples, critiques, filter_criteria)

            # 4ë‹¨ê³„: íŒ¨í‚¹
            packed_data = await self.pack_examples(passed_examples, output_format)

            # ê²°ê³¼ ì €ì¥
            output_file = self.output_dir / f"generated_{task_type}_{domain_type}_{int(time.time())}.{output_format}"
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(packed_data)

            # í†µê³„ ì •ë³´
            total_time = time.time() - start_time
            stats = {
                "total_instructions": len(instructions),
                "generated_examples": len(examples),
                "critiqued_examples": len(critiques),
                "passed_examples": len(passed_examples),
                "failed_examples": len(failed_examples),
                "pass_rate": len(passed_examples) / len(examples) * 100,
                "total_time": total_time,
                "output_file": str(output_file),
                "filter_criteria": filter_criteria.dict(),
            }

            # í†µê³„ ì €ì¥
            stats_file = self.output_dir / f"stats_{int(time.time())}.json"
            with open(stats_file, "w", encoding="utf-8") as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)

            print(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {len(passed_examples)}ê°œ ì˜ˆì œ ìƒì„±ë¨")
            print(f"   í†µê³¼ìœ¨: {stats['pass_rate']:.1f}%")
            print(f"   ì¶œë ¥ íŒŒì¼: {output_file}")
            print(f"   ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")

            return packed_data

        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


# ì‚¬ìš© ì˜ˆì œ
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # íŒŒì´í”„ë¼ì¸ ì„¤ì •
    config = PipelineConfig(
        max_tokens_input=8000,
        max_tokens_output=2048,
        temperature=0.7,
        enable_korean_checks=True,
    )

    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipeline = FourStagePipeline(config)

    # ìƒ˜í”Œ ë°ì´í„°
    instructions = [
        "ìë™ì°¨ ë³´í—˜ì˜ ë³´ìƒ ë²”ìœ„ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë³´í—˜ê¸ˆ ì²­êµ¬ ì ˆì°¨ë¥¼ ë‹¨ê³„ë³„ë¡œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.",
        "ì‹¤ì†ë³´í—˜ê³¼ íƒ€ì‹¤ë³´í—˜ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    ]

    # ìƒ˜í”Œ ì»¨í…ìŠ¤íŠ¸ (ì‹¤ì œë¡œëŠ” Dyarchyì—ì„œ ìƒì„±)
    contexts = [
        [
            ContextItem(
                content="ìë™ì°¨ ë³´í—˜ ì•½ê´€ ì œ1ì¡° ë³´ìƒ ë²”ìœ„ì— ê´€í•œ ê·œì •",
                metadata={"source": "insurance_policy", "page": 1},
            ),
            ContextItem(
                content="ë³´ìƒ ë²”ìœ„ ì˜ˆì‹œ ë° ì œì™¸ ì‚¬í•­",
                metadata={"source": "insurance_policy", "page": 2},
            ),
        ]
        for _ in instructions
    ]

    # í•„í„°ë§ ê¸°ì¤€
    filter_criteria = FilterCriteria(
        min_overall_score=7.0,
        min_korean_quality=6.0,
        max_constraint_violations=2,
        must_pass_korean_checks=True,
    )

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    try:
        result = await pipeline.run_full_pipeline(
            instructions=instructions,
            contexts=contexts,
            task_type=TaskType.QA,
            domain_type=DomainType.INSURANCE,
            filter_criteria=filter_criteria,
            output_format="yaml",
        )

        print("\nğŸ“‹ ìƒì„±ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
        print(result[:500] + "..." if len(result) > 500 else result)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
